{
 "metadata": {
  "name": "",
  "signature": "sha256:d72a65054c69cf3c5f0b32445aebb79f3c8953f141802db9988579d0b824d788"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text Analysis - Word Count and TF-IDF Implementation\n",
      "<br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load Python modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests as req\n",
      "import StringIO as st\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import nltk\n",
      "import string\n",
      "from collections import Counter\n",
      "\n",
      "# install models from nltk\n",
      "# This should bring up a GUI window showing available models to download. \n",
      "# Select the 'models' tab and click on the 'punkt' package, and under the 'corpora' tab we want to downlod the 'stopwords' package.\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#url = 'https://raw.githubusercontent.com/BDKNS/CS109Project2014/master/40100Titles.csv'\n",
      "url = 'https://github.com/BDKNS/CS109Project2014/raw/master/20Titles.csv'\n",
      "filename = req.get(url)\n",
      "document = st.StringIO(filename.content)\n",
      "titles = pd.read_csv(document)\n",
      "titles = titles.dropna()\n",
      "titles = titles.reset_index()\n",
      "print 'Number of rows: ', titles.shape[0]\n",
      "titles.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of rows:  21\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>index</th>\n",
        "      <th>id</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> t3_202wd3</td>\n",
        "      <td> I participated in one of the biggest Magic: th...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> t3_25blmh</td>\n",
        "      <td> This is Carter. He knocked on my door to ask i...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td>   t3_test</td>\n",
        "      <td> I participated in one of the massive magic tou...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4</td>\n",
        "      <td> t3_13q31c</td>\n",
        "      <td> After searching FB for people with the same na...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5</td>\n",
        "      <td>  t3_ux56w</td>\n",
        "      <td>   When I found out I could upvote by pressing 'A'</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "   index         id                                              title\n",
        "0      0  t3_202wd3  I participated in one of the biggest Magic: th...\n",
        "1      1  t3_25blmh  This is Carter. He knocked on my door to ask i...\n",
        "2      3    t3_test  I participated in one of the massive magic tou...\n",
        "3      4  t3_13q31c  After searching FB for people with the same na...\n",
        "4      5   t3_ux56w    When I found out I could upvote by pressing 'A'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# FOR TESTING\n",
      "sentence = titles.loc[0,'title']\n",
      "sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'I participated in one of the biggest Magic: the Gathering tournaments of all time this weekend. In an effort to document it, I posed for pictures near people with exposed asscracks. I present to you Grand Prix Richmond Crackstyle.'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  TESTING: do a word count of the first row of the dataframe\n",
      "\n",
      "lowers = sentence.lower()\n",
      "no_punctuation = lowers.translate(None, string.punctuation)\n",
      "tokens = nltk.word_tokenize(no_punctuation)\n",
      "\n",
      "count = Counter(tokens)\n",
      "print count.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('i', 3), ('in', 2), ('to', 2), ('of', 2), ('the', 2), ('gathering', 1), ('people', 1), ('it', 1), ('one', 1), ('prix', 1)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TESTING: remove the most common words from each row\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
      "count = Counter(filtered)\n",
      "print count.most_common(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('gathering', 1), ('document', 1), ('magic', 1), ('asscracks', 1), ('people', 1), ('exposed', 1), ('weekend', 1), ('effort', 1), ('posed', 1), ('one', 1), ('richmond', 1), ('grand', 1), ('near', 1), ('crackstyle', 1), ('prix', 1), ('time', 1), ('biggest', 1), ('pictures', 1), ('tournaments', 1), ('participated', 1), ('present', 1)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do word count for each row in dataframe\n",
      "\n",
      "from nltk.stem.porter import *\n",
      "\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "stemmer = PorterStemmer()\n",
      "most_common_words_list = []\n",
      "sentences = []\n",
      "\n",
      "for index, row in titles.iterrows():\n",
      "    sentence = row['title']\n",
      "    sentences.append(sentence)  # save the sentence into a separate list to be used to calc doc similarity\n",
      "    lowers = sentence.lower()\n",
      "    no_punctuation = lowers.translate(None, string.punctuation)\n",
      "    tokens = nltk.word_tokenize(no_punctuation)\n",
      "    \n",
      "    # remove stopwords\n",
      "    filtered = [w for w in tokens if not w in stopwords.words('english')]  \n",
      "\n",
      "    # perform stemming    \n",
      "    stemmed = stem_tokens(filtered, stemmer)\n",
      "    count = Counter(stemmed)\n",
      "    \n",
      "    most_common_words_list.append(count.most_common(100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_common_words_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 110,
       "text": [
        "[[(u'tournament', 1),\n",
        "  (u'magic', 1),\n",
        "  (u'expos', 1),\n",
        "  (u'pose', 1),\n",
        "  (u'pictur', 1),\n",
        "  (u'effort', 1),\n",
        "  (u'gather', 1),\n",
        "  (u'particip', 1),\n",
        "  (u'one', 1),\n",
        "  (u'richmond', 1),\n",
        "  (u'grand', 1),\n",
        "  (u'near', 1),\n",
        "  (u'present', 1),\n",
        "  (u'prix', 1),\n",
        "  (u'time', 1),\n",
        "  (u'biggest', 1),\n",
        "  (u'document', 1),\n",
        "  (u'weekend', 1),\n",
        "  (u'crackstyl', 1),\n",
        "  (u'peopl', 1),\n",
        "  (u'asscrack', 1)],\n",
        " [(u'knock', 1),\n",
        "  (u'door', 1),\n",
        "  (u'carter', 1),\n",
        "  (u'could', 1),\n",
        "  (u'ask', 1),\n",
        "  (u'banana', 1),\n",
        "  (u'left', 1)],\n",
        " [(u'pose', 1),\n",
        "  (u'magic', 1),\n",
        "  (u'expos', 1),\n",
        "  (u'peopl', 1),\n",
        "  (u'tournament', 1),\n",
        "  (u'particip', 1),\n",
        "  (u'one', 1),\n",
        "  (u'prix', 1),\n",
        "  (u'grand', 1),\n",
        "  (u'cracker', 1),\n",
        "  (u'massiv', 1),\n",
        "  (u'weekend', 1),\n",
        "  (u'crackstyl', 1),\n",
        "  (u'present', 1)],\n",
        " [(u'pic', 2),\n",
        "  (u'replic', 1),\n",
        "  (u'search', 1),\n",
        "  (u'name', 1),\n",
        "  (u'peopl', 1),\n",
        "  (u'make', 1),\n",
        "  (u'request', 1),\n",
        "  (u'send', 1),\n",
        "  (u'profil', 1),\n",
        "  (u'fb', 1),\n",
        "  (u'id', 1),\n",
        "  (u'friend', 1)],\n",
        " [(u'press', 1), (u'found', 1), (u'could', 1), (u'upvot', 1)],\n",
        " [(u'wont', 1),\n",
        "  (u'awkward', 1),\n",
        "  (u'situat', 1),\n",
        "  (u'rip', 1),\n",
        "  (u'label', 1),\n",
        "  (u'escap', 1)],\n",
        " [(u'photo', 1),\n",
        "  (u'someon', 1),\n",
        "  (u'us', 1),\n",
        "  (u'yesterday', 1),\n",
        "  (u'camera', 1),\n",
        "  (u'take', 1),\n",
        "  (u'ask', 1),\n",
        "  (u'found', 1),\n",
        "  (u'went', 1),\n",
        "  (u'parasail', 1),\n",
        "  (u'boat', 1)],\n",
        " [(u'ive', 1),\n",
        "  (u'use', 1),\n",
        "  (u'pictur', 1),\n",
        "  (u'professor', 1),\n",
        "  (u'univers', 1),\n",
        "  (u'profil', 1),\n",
        "  (u'3', 1),\n",
        "  (u'portal', 1),\n",
        "  (u'student', 1),\n",
        "  (u'year', 1),\n",
        "  (u'call', 1),\n",
        "  (u'final', 1),\n",
        "  (u'today', 1)],\n",
        " [(u'switzerland', 1), (u'thing', 1), (u'live', 1), (u'best', 1)],\n",
        " [(u'last', 1),\n",
        "  (u'hous', 1),\n",
        "  (u'mission', 1),\n",
        "  (u'mous', 1),\n",
        "  (u'mode', 1),\n",
        "  (u'year', 1),\n",
        "  (u'went', 1),\n",
        "  (u'imposs', 1)],\n",
        " [(u'blue', 1), (u'screen', 1), (u'death', 1)],\n",
        " [(u'advertis', 1), (u'local', 1), (u'museum', 1), (u'scienc', 1)],\n",
        " [(u'perfect', 1),\n",
        "  (u'dont', 1),\n",
        "  (u'write', 1),\n",
        "  (u'birthday', 1),\n",
        "  (u'know', 1),\n",
        "  (u'card', 1)],\n",
        " [(u'get', 1),\n",
        "  (u'win', 1),\n",
        "  (u'lotto', 1),\n",
        "  (u'pictur', 1),\n",
        "  (u'need', 1),\n",
        "  (u'amount', 1),\n",
        "  (u'minimum', 1),\n",
        "  (u'ask', 1),\n",
        "  (u'taken', 1)],\n",
        " [(u'enjoy', 1),\n",
        "  (u'photoshop', 1),\n",
        "  (u'reddit', 1),\n",
        "  (u'make', 1),\n",
        "  (u'advertis', 1),\n",
        "  (u'class', 1),\n",
        "  (u'hope', 1)],\n",
        " [(u'plug', 1), (u'work', 1), (u'usb', 1), (u'that', 1)],\n",
        " [(u'ever', 1), (u'synopsi', 1), (u'film', 1), (u'best', 1)],\n",
        " [(u'mrnoodl', 1), (u'goddammit', 1)],\n",
        " [(u'bag', 2),\n",
        "  (u'small', 2),\n",
        "  (u'unless', 1),\n",
        "  (u'fit', 1),\n",
        "  (u'canin', 1),\n",
        "  (u'sit', 1),\n",
        "  (u'system', 1),\n",
        "  (u'pitbul', 1),\n",
        "  (u'train', 1),\n",
        "  (u'york', 1),\n",
        "  (u'calmli', 1),\n",
        "  (u'ban', 1),\n",
        "  (u'new', 1),\n",
        "  (u'guy', 1),\n",
        "  (u'subway', 1)],\n",
        " [(u'wingman', 1), (u'fb', 1), (u'spontan', 1), (u'year', 1)],\n",
        " [(u'cnn', 1), (u'made', 1), (u'hey', 1), (u'shittywatercolor', 1)]]"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 111,
       "text": [
        "['I participated in one of the biggest Magic: the Gathering tournaments of all time this weekend. In an effort to document it, I posed for pictures near people with exposed asscracks. I present to you Grand Prix Richmond Crackstyle.',\n",
        " 'This is Carter. He knocked on my door to ask if he could have a banana then left.',\n",
        " 'I participated in one of the massive magic tournament this weekend.  I posed for people with exposed crackers. I present to her Crackstyle Grand Prix.',\n",
        " \"After searching FB for people with the same name as me, I'd replicate their profile pic, make it my own and send them a friend request. Here are the pics\",\n",
        " \"When I found out I could upvote by pressing 'A'\",\n",
        " \"You won't escape an awkward situation by ripping off this label.\",\n",
        " 'We went parasailing yesterday and asked someone on the boat to take photos of us. We found this on our camera.',\n",
        " \"I've been using this as my profile picture for my University's student portal for 3 years. Finally had a professor call me out on it today.\",\n",
        " 'The best thing about living in Switzerland',\n",
        " 'A mouse that went into Mission Impossible mode in my house last year.',\n",
        " 'Blue screen of death.',\n",
        " 'Advertisements for my local Science Museum',\n",
        " \"The perfect birthday card For when you don't know what to write\",\n",
        " 'I asked the lotto what is the minimum winning amount needed to get your picture taken',\n",
        " 'I had to make an advertisement for my photoshop class. I hope reddit enjoys! ',\n",
        " \"So THAT'S how USB plugs work.\",\n",
        " 'The Best Film Synopsis Ever',\n",
        " 'Goddammit Mr.Noodle!',\n",
        " 'The New York subway system bans canines unless they can fit in a small bag, so this guy trained his pit-bull to calmly sit in his small bag.',\n",
        " 'Spontaneous Wingman of the Year [fb]',\n",
        " 'Hey, Shitty_Watercolor made it to CNN!']"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reference: tfidf implementation in scikits-learn credit to: http://stackoverflow.com/a/8897648\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "mydoclist = sentences \n",
      "\n",
      "# Each document is represented as a word vector in the vector space model.\n",
      "# We represent a comparison of the angles between these vectors in a cosine dist matrix.\n",
      "# The resulting angle is an estimate of similarity based on tf-idf weighted word vectors.\n",
      "# For instance, a distance of 1 means that we are comparing the document to itself, so of course it has a similarity of 1, \n",
      "# since the cosine of an angle with 0 degrees is 1\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
      "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
      "\n",
      "document_distances = (tfidf_matrix * tfidf_matrix.T)\n",
      "print 'Created a ' + str(document_distances.get_shape()[0]) + ' by ' + str(document_distances.get_shape()[1]) + ' document-document cosine distance matrix.'\n",
      "print document_distances.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Created a 21 by 21 document-document cosine distance matrix.\n",
        "[[ 1.          0.04223973  0.60863008  0.12274537  0.          0.0992402\n",
        "   0.09747842  0.06635152  0.1134177   0.05395637  0.09259317  0.03023785\n",
        "   0.12709461  0.08270327  0.09528994  0.          0.04654435  0.\n",
        "   0.1083229   0.12869788  0.09322081]\n",
        " [ 0.04223973  1.          0.04002201  0.020553    0.07945357  0.02840214\n",
        "   0.10530735  0.09851258  0.          0.03102009  0.          0.04542369\n",
        "   0.02285891  0.07432563  0.05528801  0.          0.          0.\n",
        "   0.02777695  0.          0.03137201]\n",
        " [ 0.60863008  0.04002201  1.          0.11272975  0.          0.02936504\n",
        "   0.0766376   0.06012282  0.07745285  0.03684676  0.0632318   0.0412988\n",
        "   0.07471524  0.05647799  0.05309477  0.          0.03178509  0.\n",
        "   0.08199358  0.08788767  0.0324356 ]\n",
        " [ 0.12274537  0.020553    0.11272975  1.          0.          0.\n",
        "   0.05910739  0.20150045  0.04403815  0.02593201  0.          0.07136572\n",
        "   0.05842945  0.05862595  0.10430399  0.          0.05140047  0.\n",
        "   0.01837603  0.12801674  0.05049443]\n",
        " [ 0.          0.07945357  0.          0.          1.          0.10437753\n",
        "   0.07158492  0.07123516  0.          0.          0.          0.\n",
        "   0.10627666  0.          0.          0.          0.          0.          0.\n",
        "   0.          0.        ]\n",
        " [ 0.0992402   0.02840214  0.02936504  0.          0.10437753  1.\n",
        "   0.02558935  0.02546432  0.          0.          0.          0.\n",
        "   0.06868242  0.          0.0681199   0.          0.          0.\n",
        "   0.02038057  0.          0.        ]\n",
        " [ 0.09747842  0.10530735  0.0766376   0.05910739  0.07158492  0.02558935\n",
        "   1.          0.08061023  0.02373089  0.05438649  0.05510158  0.\n",
        "   0.03905357  0.09855666  0.02042641  0.          0.02769823  0.\n",
        "   0.03492838  0.07658726  0.02826509]\n",
        " [ 0.06635152  0.09851258  0.06012282  0.20150045  0.07123516  0.02546432\n",
        "   0.08061023  1.          0.          0.05562295  0.          0.15307613\n",
        "   0.05185528  0.04909942  0.16682147  0.          0.          0.\n",
        "   0.01390925  0.          0.05415409]\n",
        " [ 0.1134177   0.          0.07745285  0.04403815  0.          0.\n",
        "   0.02373089  0.          1.          0.06128615  0.          0.\n",
        "   0.03523144  0.06029889  0.          0.          0.21800451  0.\n",
        "   0.08861063  0.05139715  0.        ]\n",
        " [ 0.05395637  0.03102009  0.03684676  0.02593201  0.          0.\n",
        "   0.05438649  0.05562295  0.06128615  1.          0.          0.05731172\n",
        "   0.          0.          0.04115247  0.10315586  0.          0.\n",
        "   0.05114639  0.11779207  0.        ]\n",
        " [ 0.09259317  0.          0.0632318   0.          0.          0.\n",
        "   0.05510158  0.          0.          0.          1.          0.          0.\n",
        "   0.          0.          0.          0.          0.          0.\n",
        "   0.11934084  0.        ]\n",
        " [ 0.03023785  0.04542369  0.0412988   0.07136572  0.          0.          0.\n",
        "   0.15307613  0.          0.05731172  0.          1.          0.05342966\n",
        "   0.          0.11325291  0.          0.          0.          0.          0.\n",
        "   0.        ]\n",
        " [ 0.12709461  0.02285891  0.07471524  0.05842945  0.10627666  0.06868242\n",
        "   0.03905357  0.05185528  0.03523144  0.          0.          0.05342966\n",
        "   1.          0.14631954  0.0686905   0.          0.04112145  0.\n",
        "   0.03110411  0.03997799  0.04196302]\n",
        " [ 0.08270327  0.07432563  0.05647799  0.05862595  0.          0.\n",
        "   0.09855666  0.04909942  0.06029889  0.          0.          0.\n",
        "   0.14631954  1.          0.0259512   0.          0.0703797   0.\n",
        "   0.03919809  0.06842265  0.03591003]\n",
        " [ 0.09528994  0.05528801  0.05309477  0.10430399  0.          0.0681199\n",
        "   0.02042641  0.16682147  0.          0.04115247  0.          0.11325291\n",
        "   0.0686905   0.0259512   1.          0.          0.          0.\n",
        "   0.01626856  0.          0.04161934]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.10315586  0.          0.          0.          0.\n",
        "   0.          1.          0.          0.          0.05866757  0.          0.        ]\n",
        " [ 0.04654435  0.          0.03178509  0.05140047  0.          0.\n",
        "   0.02769823  0.          0.21800451  0.          0.          0.\n",
        "   0.04112145  0.0703797   0.          0.          1.          0.\n",
        "   0.02206019  0.05998975  0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          1.          0.          0.          0.        ]\n",
        " [ 0.1083229   0.02777695  0.08199358  0.01837603  0.          0.02038057\n",
        "   0.03492838  0.01390925  0.08861063  0.05114639  0.          0.\n",
        "   0.03110411  0.03919809  0.01626856  0.05866757  0.02206019  0.          1.\n",
        "   0.02144676  0.02251166]\n",
        " [ 0.12869788  0.          0.08788767  0.12801674  0.          0.\n",
        "   0.07658726  0.          0.05139715  0.11779207  0.11934084  0.\n",
        "   0.03997799  0.06842265  0.          0.          0.05998975  0.\n",
        "   0.02144676  1.          0.        ]\n",
        " [ 0.09322081  0.03137201  0.0324356   0.05049443  0.          0.\n",
        "   0.02826509  0.05415409  0.          0.          0.          0.\n",
        "   0.04196302  0.03591003  0.04161934  0.          0.          0.\n",
        "   0.02251166  0.          1.        ]]\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# find the most similar documents, ranked in order from most similar to least similar\n",
      "\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "\n",
      "#code taken from here: http://stackoverflow.com/a/12128777\n",
      "from sklearn.metrics.pairwise import linear_kernel \n",
      "\n",
      "#linear kernel is the same as cosine distance when using tfidf + euclidean normalized vectors (L2 Norm=1))\n",
      "#this is the benefit of sticking to scikits-learn from beginning to end of an analysis\n",
      "\n",
      "cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten() # similarity to the first document in the list\n",
      "\n",
      "related_docs_indices = cosine_similarities.argsort()[:-len(mydoclist)-1:-1] # order of most to least similar\n",
      "\n",
      "print 'document list to be analyzed: \\n', mydoclist, '\\n'\n",
      "\n",
      "print 'cosine similarity matrix, ordered by most to least similar: \\n', cosine_similarities[related_docs_indices], '\\n' # cosine distances to the first document in the list\n",
      "\n",
      "print 'indices of the documents closest related to the first one in the list: \\n', related_docs_indices, '\\n'\n",
      "\n",
      "print 'document we want to compare: \\n', mydoclist[related_docs_indices[0]], '\\n'\n",
      "\n",
      "print '3 most similar documents to the one above:'\n",
      "print '#1: SIMILARITY SCORE: ', cosine_similarities[related_docs_indices[1]], '\\n', mydoclist[related_docs_indices[1]], '\\n'\n",
      "print '#2: SIMILARITY SCORE: ', cosine_similarities[related_docs_indices[2]], '\\n',mydoclist[related_docs_indices[2]], '\\n'\n",
      "print '#3: SIMILARITY SCORE: ', cosine_similarities[related_docs_indices[3]], '\\n', mydoclist[related_docs_indices[3]], '\\n'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "document list to be analyzed: \n",
        "['I participated in one of the biggest Magic: the Gathering tournaments of all time this weekend. In an effort to document it, I posed for pictures near people with exposed asscracks. I present to you Grand Prix Richmond Crackstyle.', 'This is Carter. He knocked on my door to ask if he could have a banana then left.', 'I participated in one of the massive magic tournament this weekend.  I posed for people with exposed crackers. I present to her Crackstyle Grand Prix.', \"After searching FB for people with the same name as me, I'd replicate their profile pic, make it my own and send them a friend request. Here are the pics\", \"When I found out I could upvote by pressing 'A'\", \"You won't escape an awkward situation by ripping off this label.\", 'We went parasailing yesterday and asked someone on the boat to take photos of us. We found this on our camera.', \"I've been using this as my profile picture for my University's student portal for 3 years. Finally had a professor call me out on it today.\", 'The best thing about living in Switzerland', 'A mouse that went into Mission Impossible mode in my house last year.', 'Blue screen of death.', 'Advertisements for my local Science Museum', \"The perfect birthday card For when you don't know what to write\", 'I asked the lotto what is the minimum winning amount needed to get your picture taken', 'I had to make an advertisement for my photoshop class. I hope reddit enjoys! ', \"So THAT'S how USB plugs work.\", 'The Best Film Synopsis Ever', 'Goddammit Mr.Noodle!', 'The New York subway system bans canines unless they can fit in a small bag, so this guy trained his pit-bull to calmly sit in his small bag.', 'Spontaneous Wingman of the Year [fb]', 'Hey, Shitty_Watercolor made it to CNN!'] \n",
        "\n",
        "cosine similarity matrix, ordered by most to least similar: \n",
        "[ 1.          0.60863008  0.12869788  0.12709461  0.12274537  0.1134177\n",
        "  0.1083229   0.0992402   0.09747842  0.09528994  0.09322081  0.09259317\n",
        "  0.08270327  0.06635152  0.05395637  0.04654435  0.04223973  0.03023785\n",
        "  0.          0.          0.        ] \n",
        "\n",
        "indices of the documents closest related to the first one in the list: \n",
        "[ 0  2 19 12  3  8 18  5  6 14 20 10 13  7  9 16  1 11 15  4 17] \n",
        "\n",
        "document we want to compare: \n",
        "I participated in one of the biggest Magic: the Gathering tournaments of all time this weekend. In an effort to document it, I posed for pictures near people with exposed asscracks. I present to you Grand Prix Richmond Crackstyle. \n",
        "\n",
        "3 most similar documents to the one above:\n",
        "#1: SIMILARITY SCORE:  0.608630076099 \n",
        "I participated in one of the massive magic tournament this weekend.  I posed for people with exposed crackers. I present to her Crackstyle Grand Prix. \n",
        "\n",
        "#2: SIMILARITY SCORE:  0.128697882315 \n",
        "Spontaneous Wingman of the Year [fb] \n",
        "\n",
        "#3: SIMILARITY SCORE:  0.127094611551 \n",
        "The perfect birthday card For when you don't know what to write \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do some kind of plot here to show the documents most similar\n",
      "###\n",
      "###\n",
      "###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}